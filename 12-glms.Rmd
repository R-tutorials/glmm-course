# GLM crash course

# Goals

- Gain familiarity and intuition with common GLMs

# Data

We are going to simulate our own data for this exercise. The following will be common throughout: `N` is the number of data points. `x` is our predictor variable. `a` is the intercept, and `b` is the slope.

```{r}
library(ggplot2)
library(dplyr)
set.seed(111)
N <- 200
x <- runif(N, -1, 1)
a <- 0.5
b <- 1.3
d <- data_frame(x = x)
d
```

So our data frame includes a column for one predictor, `x`. This can represent any variable that you might try to predict some other process with. 

We will define the following functions to make our code easier to read later:

```{r}
logit <- function(x) qlogis(x)
inverse_logit <- function(x) plogis(x)
```

# Link functions 

Remember that there are two components to a GLM: a link function that describes what is happening with the mean of the data, and an error distribution that describes the variability around the mean. 

Put another way, the "link" is the transformation you need to apply to make the (mean of the) response data linear with respect to the predictors.

The error distribution describes the spread of the data around the raw untransformed mean.

The two most common link functions, and the only two we are going to work with in this workshop, are the log and logit links.

Let's look at those now. So if we want to fit a curve that looks like this:

```{r}
xx <- seq(0, 5, length.out = 200)
plot(xx, exp(xx), type = "l")
```

A log link will make the data look linear:

```{r}
plot(xx, log(exp(xx)), type = "l")
```

And if this is the curve we ultimately want to fit:

```{r}
xx <- seq(-5, 5, length.out = 200)
plot(xx, inverse_logit(xx), type = "l")
```

Then we can make it linear by applying the logit link:

```{r}
plot(xx, logit(inverse_logit(xx)), type = "l")
```

There are many ways you can specify a distribution family and link function in R. I'm going to try and be consistent and specify them like this `family(link = "link")`.

When in doubt, read the help for `?family`.

# Common GLMs

## Gamma, log link

The Gamma distribution combined with a log link is commonly used to model continuous positive data. (Therefore, this can often be used interchangeably with a linear regression where the response data are log transformed.)

Here, and throughout, we will generate a true set of response data `y_true`. We will then add variability around the true values according to each distribution that we work with. 

```{r}
y_true <- exp(a + b * x)
shape <- 8
y <- rgamma(N, rate = shape / y_true, shape = shape)
d$y <- y
plot(x, y);lines(sort(x), y_true[order(x)])
```

What are some examples of data sets that might resemble this?

Let's fit a GLM that reflects these data.

```{r}
(m_gamma <- glm(y ~ x, family = 
    Gamma(link = "log"), data = d)) # exercise

ggeffects::ggpredict(m_gamma, "x") %>%
  ggplot(aes(x, predicted)) +
  geom_line(colour = "red") + 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  geom_point(data = d, aes(x = x, y = y))  # plot simulated observations on top of predictions
```

## Poisson, log link

The Poisson distribution with a log link is commonly used to model count data or any data where the response is a whole number. The Poisson distribution assumes that the variance scales one-to-one with the mean.

```{r}
y_true <- exp(a + b * x)
y <- rpois(N, lambda = y_true)
d$y <- y
plot(x, y);lines(sort(x), y_true[order(x)])
```

What are some examples of data sets that might resemble this?

```{r}
(m_poisson <- glm(y ~ x, family = 
    poisson(link = "log"), data = d)) # exercise

ggeffects::ggpredict(m_poisson, "x") %>%
  ggplot(aes(x, predicted)) +
  geom_line(colour = "red") + 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  geom_point(data = d, aes(x = x, y = y))
```

## Negative binomial, log link

The negative binomial distribution with a log link can also model count data but allows the variance to grow as a quadratic function of the mean. In real data sets, it's probably more common to see the negative binomial than the Poisson.

```{r}
y_true <- exp(a + b * x)
y <- MASS::rnegbin(N, mu = y_true, theta = 0.6)
d$y <- y
plot(x, y);lines(sort(x), y_true[order(x)])
```

Notice the much larger values on the right side of the graph.

(Also note that there is another common parameterization of the negative binomial which allows the variance to grow linearly with the mean.)

We have to use a special function to fit the negative binomial GLM in R:

```{r}
(m_nb <- MASS::glm.nb(y ~ x, data = d))

ggeffects::ggpredict(m_nb, "x") %>%
  ggplot(aes(x, predicted)) +
  geom_line(colour = "red") + 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  geom_point(data = d, aes(x = x, y = y))
```

## Binomial, logit link

We can use a binomial response and logit link if we have response data represented by 0s and 1s. This is commonly referred to as logistic regression. 

```{r}
y_linear <- a + b * x
prob_true <- inverse_logit(y_linear)
y <- rbinom(N, 1, prob_true)
d$y <- y
plot(x, jitter(y, 0.1));lines(sort(x), prob_true[order(x)])
```

What does the true probabilities line indicate in the above plot and how does it correspond to the dots?

In what scenario might you see this kind of data? 

```{r}
(m_bin <- glm(y ~ x, family = 
    binomial(link = "logit"), data = d)) # exercise

g <- ggeffects::ggpredict(m_bin, "x") %>%
  ggplot(aes(x, predicted)) +
  geom_line(colour = "red") + 
  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .2) +
  geom_point(data = d, aes(x = x, y = y))
g

coef(m_bin)
(slope <- round(coef(m_bin)[[2]], 3)) # slope
(int <- round(coef(m_bin)[[1]], 3)) # intercept
```

How do we interpret the slope coefficient? A unit increase in `x` corresponds to a `r slope` increase in the log odds of a `1` being observed.

That means the logit-transformed true probabilities follow our estimated intercept and slope:

```{r}
plot(sort(x), logit(prob_true[order(x)]), ylab = "log odds")
abline(a = a, b = b, col = "red")
abline(a = int, b = slope, col = "blue")
```

If we exponentiate the slope coefficient we get the expected fold increase in the *odds* of observing a `1`: `r exp(slope)` per unit increase in `x`.

```{r}
plot(sort(x), exp(logit(prob_true[order(x)])), ylab = "odds")
curve(exp(int + slope * x), min(x), max(x), add = TRUE, col = "blue")
```

Of course, most people have trouble wrapping their heads around odds and log odds, but those are the only scales on which our slope is constant.

If we want to show that in terms of probability then we need to pick 2 values to compare or plot out the function as we did above. The relationship is not linear on the probability scale.

A quick trick is to take the slope of the logistic regression and divide it by 4. This will give you approximately the expected change in probability per unit change in the x variable at the steepest part of the line.

Here's a quick illustration of that:

```{r}
(approximate_slope <- slope/4)
intercept <- inverse_logit(int)

g + geom_vline(xintercept = c(-0.5, 0, 0.5), lty = 2) +
  geom_abline(intercept = intercept, 
    slope = approximate_slope, size = 1) +
  ylab("Probability of y = 1")
```

For more details on the divide-by-4 trick see: Gelman, A., and J. Hill. 2006. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press, Cambridge, UK.
